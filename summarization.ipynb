{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MOsHUjgdIrIW",
    "outputId": "f84a093e-147f-470e-aad9-80fb51193c8e",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (1.8.0)\n",
      "Requirement already satisfied: rouge-score in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (0.0.4)\n",
      "Requirement already satisfied: nltk in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (3.6.2)\n",
      "Requirement already satisfied: dataclasses in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from datasets) (0.8)\n",
      "Requirement already satisfied: pandas in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from datasets) (1.19.2)\n",
      "Requirement already satisfied: importlib-metadata in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from datasets) (4.5.0)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from datasets) (0.0.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from datasets) (4.49.0)\n",
      "Requirement already satisfied: packaging in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from datasets) (20.9)\n",
      "Requirement already satisfied: fsspec in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from datasets) (2021.6.0)\n",
      "Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: xxhash in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: dill in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: filelock in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: absl-py in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from rouge-score) (0.13.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from rouge-score) (1.15.0)\n",
      "Requirement already satisfied: joblib in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: click in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from importlib-metadata->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from importlib-metadata->datasets) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages (from pandas->datasets) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets rouge-score nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning a model on a summarization task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTCFado4IrIc"
   },
   "source": [
    "In this notebook, 怎么fine-tune一个summarization摘要模型[🤗 Transformers](https://github.com/huggingface/transformers). 数据集是[XSum dataset](https://arxiv.org/pdf/1808.08745.pdf)，包含BBC新闻和新闻摘要.\n",
    "\n",
    "使用 🤗 Datasets 加载数据和 `Trainer` API去fine-tune数据.\n",
    "\n",
    "[Model Hub](https://huggingface.co/models) 中的模型有sequence-to-sequence都可以. 比如[`t5-small`](https://huggingface.co/t5-small). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "# 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset xsum (/raid/wuxiaojun/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"xsum\")\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GWiVUF0jIrIv",
    "outputId": "35e3ea43-f397-4a54-c90c-f2cf8d36873e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'id', 'summary'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'id', 'summary'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'id', 'summary'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下载下来数据集长这样，20w+训练集，1w+验证集和测试集\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 'Recent reports have linked some France-based players with returns to Wales.\\n\"I\\'ve always felt - and this is with my rugby hat on now; this is not region or WRU - I\\'d rather spend that money on keeping players in Wales,\" said Davies.\\nThe WRU provides £2m to the fund and £1.3m comes from the regions.\\nFormer Wales and British and Irish Lions fly-half Davies became WRU chairman on Tuesday 21 October, succeeding deposed David Pickering following governing body elections.\\nHe is now serving a notice period to leave his role as Newport Gwent Dragons chief executive after being voted on to the WRU board in September.\\nDavies was among the leading figures among Dragons, Ospreys, Scarlets and Cardiff Blues officials who were embroiled in a protracted dispute with the WRU that ended in a £60m deal in August this year.\\nIn the wake of that deal being done, Davies said the £3.3m should be spent on ensuring current Wales-based stars remain there.\\nIn recent weeks, Racing Metro flanker Dan Lydiate was linked with returning to Wales.\\nLikewise the Paris club\\'s scrum-half Mike Phillips and centre Jamie Roberts were also touted for possible returns.\\nWales coach Warren Gatland has said: \"We haven\\'t instigated contact with the players.\\n\"But we are aware that one or two of them are keen to return to Wales sooner rather than later.\"\\nSpeaking to Scrum V on BBC Radio Wales, Davies re-iterated his stance, saying keeping players such as Scarlets full-back Liam Williams and Ospreys flanker Justin Tipuric in Wales should take precedence.\\n\"It\\'s obviously a limited amount of money [available]. The union are contributing 60% of that contract and the regions are putting £1.3m in.\\n\"So it\\'s a total pot of just over £3m and if you look at the sorts of salaries that the... guys... have been tempted to go overseas for [are] significant amounts of money.\\n\"So if we were to bring the players back, we\\'d probably get five or six players.\\n\"And I\\'ve always felt - and this is with my rugby hat on now; this is not region or WRU - I\\'d rather spend that money on keeping players in Wales.\\n\"There are players coming out of contract, perhaps in the next year or so… you\\'re looking at your Liam Williams\\' of the world; Justin Tipuric for example - we need to keep these guys in Wales.\\n\"We actually want them there. They are the ones who are going to impress the young kids, for example.\\n\"They are the sort of heroes that our young kids want to emulate.\\n\"So I would start off [by saying] with the limited pot of money, we have to retain players in Wales.\\n\"Now, if that can be done and there\\'s some spare monies available at the end, yes, let\\'s look to bring players back.\\n\"But it\\'s a cruel world, isn\\'t it?\\n\"It\\'s fine to take the buck and go, but great if you can get them back as well, provided there\\'s enough money.\"\\nBritish and Irish Lions centre Roberts has insisted he will see out his Racing Metro contract.\\nHe and Phillips also earlier dismissed the idea of leaving Paris.\\nRoberts also admitted being hurt by comments in French Newspaper L\\'Equipe attributed to Racing Coach Laurent Labit questioning their effectiveness.\\nCentre Roberts and flanker Lydiate joined Racing ahead of the 2013-14 season while scrum-half Phillips moved there in December 2013 after being dismissed for disciplinary reasons by former club Bayonne.',\n",
       " 'id': '29750031',\n",
       " 'summary': 'New Welsh Rugby Union chairman Gareth Davies believes a joint £3.3m WRU-regions fund should be used to retain home-based talent such as Liam Williams, not bring back exiled stars.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "# 随机展示一些数据\n",
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>id</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Michael Cockerell told reporters about the plan at a press screening of his new series Inside the Commons.\\n\"I'm not fingering anyone by name,\" Mr Cockerell said, when asked who was involved in the plot.\\nBut he did say they were \"right wing Tories... what Downing Street know as the berserkers, the naughty bench\".\\nHe declined to name the cameraman who was the subject of the apparent skulduggery.\\nIn the first episode of the four-part series, to be shown on Tuesday, the Conservative MP Bill Wiggin is seen complaining to the Speaker during a session in the Commons about the presence of camera crews in the chamber itself.\\nMichael Cockerell said Mr Wiggin was not involved in the plot.\\n\"We heard of a plan to knock over the cameraman and cause the House to be suspended, and then they would blame it on us and suggest we shouldn't be there,\" he said, adding that Parliamentary staff had let them know about the plot and had managed to prevent it from happening.\\nHe said there were \"very few\" opponents to the documentary, but \"in Parliament every day there are cunning plans, it is a place made for plotting and conspiracy\".\\nThe documentary was filmed over the course of a year - after six years of attempting to persuade the parliamentary authorities to allow them the access they required.\\nAtlantic Productions, the producers of the series for the BBC, gathered 600 hours of raw material for the four hours that will be broadcast throughout February.\\nThe first episode is broadcast on Tuesday on BBC Two at 21:00 GMT.</td>\n",
       "      <td>31039104</td>\n",
       "      <td>MPs plotted to knock over a BBC cameraman in the House of Commons - in the hope of stopping a new documentary on Westminster life, a film-maker says.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Â£2.5m is being invested in new facilities at Liberton High where 12-year-old Keane Wallis Bennett died after the \"modesty\" wall fell on her.\\nEdinburgh City Council said a permanent memorial was planned as well as a new extension to the PE block.\\nThe Health and Safety Executive is continuing its investigation into the tragedy, which happened on 1 April.\\nPaul Godzik, Edinburgh City Council's education convener, said: \"The overwhelming view from staff at the school, parents and the local community is that the gym should be demolished as soon as possible.\\n\"The proposal is that this will happen at the earliest opportunity, and assuming that the necessary consents are in place, we hope this will be able to take place over the summer break to minimise disruption to the school.\\n\"The tragic incident two months ago has obviously had a devastating effect on the local community and we are determined to work with them and other partners to ensure nothing similar can ever happen again.\\n\"Discussions about a suitable memorial at the school for Keane are continuing and we hope to make an announcement in the near future.\"</td>\n",
       "      <td>28015018</td>\n",
       "      <td>The gym hall where an Edinburgh schoolgirl died when a wall collapsed is to be demolished this summer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Katrina O'Hara, 44, was stabbed at Jocks Barbers in East Street, Blandford Forum, on 7 January.\\nDorset Police said it referred itself to the Independent Police Complaints Commission (IPCC) as it had \"prior contact with people involved\".\\nStuart Thomas, 49, who has been charged with murdering Ms O'Hara, is due before Winchester Crown Court on 1 April.\\nAn IPCC spokeswoman said: \"The IPCC has begun an independent investigation into previous contact between Dorset Police and Katrina O'Hara, and with Stuart Thomas, also known as George Thomas.\"</td>\n",
       "      <td>35350406</td>\n",
       "      <td>A police watchdog is to investigate circumstances relating to the suspected murder of a Dorset hairdresser.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michael Cole, 29, of Newhaven, East Sussex, admitted inciting sexual activity with boys and possessing indecent images of children.\\nHe was charged after police appealed to teenagers who may have been threatened online by a man posing as a woman.\\nCole was bailed at Lewes Crown Court until sentencing on 19 June.\\nDuring its investigation, Sussex Police said teenagers, mainly boys aged 13 to 17 in the Seaford, Newhaven and Peacehaven areas, were approached by a person calling themselves Jenny Lane and threatened if they did not send inappropriate images.\\nDet Con Steve Shimmons said Cole, aka \"Jenny Lane\", used social media, including Facebook and Skype, to contact teenagers and sent out more than 100 friend requests.</td>\n",
       "      <td>32629489</td>\n",
       "      <td>A man has pleaded guilty to sex offences against boys after he tricked teenagers into sending him \"inappropriate\" images online.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jean Brooks uses the hairdryer as a pretend speed gun outside her home in Hucknall, Nottinghamshire.\\nA video of her has been viewed more than 33 million times on the BBC Radio Nottingham Facebook page.\\nSince her new-found fame, she said she is asked for selfies and fans visit her home.\\n\"People come up to me and hug me and say 'thank you for making the street safer',\" said the 63-year-old.\\n\"As far as the local school kids are concerned, I'm a national hero.\\n\"If the traffic slows down nobody's going to get hurt. I've already got a cat that's only got three legs because he didn't know his green cross code.\"\\nThe video has since spawned memes quoting parts of the interview, in which Ms Brooks said: \"If we can't be safe in our own streets, how the hell are we going to be safe in the world?\"\\nPeople have also quoted the \"neooooow, neooooow, neoooow\" noise she made when imitating the sound of vehicles speeding past her home.\\nA hitchhiker from Germany turned up at her home with a gift on Wednesday after seeing the hairdryer video.\\n\"I nearly burst into tears when I saw him,\" she said.\\n\"I hugged him so hard I thought I was going to crack his ribs, bless him.\"\\nUpdates on this story and more from the East Midlands\\nBartek Zabel, from Hamburg, said: \"It's brilliant because you don't need expensive equipment to slow the drivers down.\"\\nHe has sent the video to his friends and said they loved it too.\\n\"I just had to visit her because she's like a local hero and like a celebrity,\" he added.\\nA man who viewed the hairdryer video also delivered a van of nearly 200 toys to her home on Saturday night, which she gave to children on her estate.\\n\"It's not often I'm made speechless but I was speechless,\" said Ms Brooks.\\nMs Brooks, who is a biker herself, contacted the BBC after it published a video of irresponsible quad bikers and motorcyclists who rode dangerously in Nottingham city centre.\\nShe claimed the gang of bikers - who were caught on CCTV doing wheelies, driving the wrong way and weaving in and out of traffic - regularly went down her road.\\nShe said she wanted the hairdryer to become symbolic of communities taking back their streets.\\nShe encouraged people to vote - particularly young people - and said they should take a hairdryer with them when they do.\\n\"Don't say anything, don't do anything, just carry a hairdryer,\" she said.\\nMs Brooks does other work for her community, including running a charity cafe in her garden to encourage people to talk to each other.\\nIn 2015, she collected Easter eggs then delivered them to children's homes and hospitals with the help of a group of bikers.</td>\n",
       "      <td>40184757</td>\n",
       "      <td>A hairdryer-wielding grandmother who became an internet sensation says schoolchildren in her area now consider her a \"national hero\".</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnjDIuQ3IrI-"
   },
   "source": [
    "# ROUGE评测指标 [`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5o4rUteaIrI_",
    "outputId": "18038ef5-554c-45c5-e00a-133b02ec10f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"rouge\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\n",
       "Calculates average rouge scores for a list of hypotheses and references\n",
       "Args:\n",
       "    predictions: list of predictions to score. Each predictions\n",
       "        should be a string with tokens separated by spaces.\n",
       "    references: list of reference for each prediction. Each\n",
       "        reference should be a string with tokens separated by spaces.\n",
       "    rouge_types: A list of rouge types to calculate.\n",
       "        Valid names:\n",
       "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
       "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
       "        `\"rougeLSum\"`: rougeLsum splits text using `\"\n",
       "\"`.\n",
       "        See details in https://github.com/huggingface/datasets/issues/617\n",
       "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
       "    use_agregator: Return aggregates if this is set to True\n",
       "Returns:\n",
       "    rouge1: rouge_1 (precision, recall, f1),\n",
       "    rouge2: rouge_2 (precision, recall, f1),\n",
       "    rougeL: rouge_l (precision, recall, f1),\n",
       "    rougeLsum: rouge_lsum (precision, recall, f1)\n",
       "Examples:\n",
       "\n",
       "    >>> rouge = datasets.load_metric('rouge')\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
       "    >>> print(results[\"rouge1\"])\n",
       "    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n",
       "    >>> print(results[\"rouge1\"].mid.fmeasure)\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rouge_1：unigram匹配情况\n",
    "# rouge_2：bigram匹配情况\n",
    "# rouge_l: 最长公共子序列\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAWdqcUBIrJC"
   },
   "source": [
    "You can call its `compute` method with your predictions and labels, which need to be list of decoded strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6XN1Rq0aIrJC",
    "outputId": "a4405435-a8a9-41ff-9f79-a13077b587c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_preds = [\"hello there\", \"general kenobi\"]\n",
    "fake_labels = [\"hello there\", \"general kenobi\"]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "# 处理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    " `AutoTokenizer.from_pretrained` 的好处:\n",
    "\n",
    "- 我们得到一个与我们要使用的模型架构相对应的Tokenizer,\n",
    "- 我们下载预训练这个checkpoint时使用的词汇表.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"t5-small\"\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "a5hBlsrHIrJL",
    "outputId": "acdaa98a-a8cd-4a20-89b8-cc26437bbe90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [8774, 6, 48, 80, 7142, 55, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this one sentence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the targets for our model, we need to tokenize them inside the `as_target_tokenizer` context manager. This will make sure the tokenizer uses the special tokens corresponding to the targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "T5模型需要加上前缀 \"summarize:\" (也可以做translate任务)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True) # truncation截断\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lm8ozrJIrJR"
   },
   "source": [
    "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "-b70jh26IrJS",
    "outputId": "acd3a42d-985b-44ee-9daa-af5d944ce1d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[21603, 10, 17716, 2279, 43, 5229, 128, 1410, 18, 390, 1508, 28, 5146, 12, 10256, 5, 96, 196, 31, 162, 373, 1800, 3, 18, 11, 48, 19, 28, 82, 22209, 3, 547, 30, 230, 117, 48, 19, 59, 1719, 42, 549, 8503, 3, 18, 27, 31, 26, 1066, 1492, 24, 540, 30, 2627, 1508, 16, 10256, 976, 243, 28571, 5, 37, 549, 8503, 795, 17586, 51, 12, 8, 3069, 11, 3996, 13606, 51, 639, 45, 8, 6266, 5, 18263, 10256, 11, 2390, 11, 7262, 10371, 7, 3971, 18, 17114, 28571, 1632, 549, 8503, 13404, 30, 2818, 1401, 1797, 6, 7229, 53, 20, 12151, 1955, 8356, 49, 53, 826, 3, 19585, 643, 9768, 5, 216, 19, 230, 3122, 3, 9, 2103, 1059, 12, 1175, 112, 1075, 38, 24260, 350, 16103, 10282, 7, 5752, 4297, 227, 271, 3, 11060, 30, 12, 8, 549, 8503, 1476, 16, 1600, 5, 28571, 47, 859, 8, 1374, 5638, 859, 10282, 7, 6, 411, 7, 2026, 63, 7, 6, 14586, 7677, 11, 26911, 2419, 7, 4298, 113, 130, 10960, 52, 26786, 16, 3, 9, 813, 11674, 11044, 28, 8, 549, 8503, 24, 3492, 16, 3, 9, 3996, 3328, 51, 1154, 16, 1660, 48, 215, 5, 86, 8, 7178, 13, 24, 1154, 271, 612, 6, 28571, 243, 8, 3996, 19660, 51, 225, 36, 1869, 30, 3, 5833, 750, 10256, 18, 390, 4811, 2367, 132, 5, 86, 1100, 1274, 6, 16046, 10730, 24397, 49, 2744, 31914, 17, 15, 47, 5229, 28, 7646, 12, 10256, 5, 3, 21322, 8, 1919, 1886, 31, 7, 14667, 440, 18, 17114, 4794, 16202, 7, 11, 2050, 17845, 2715, 7, 130, 92, 2633, 26, 21, 487, 5146, 5, 10256, 3763, 16700, 2776, 17, 40, 232, 65, 243, 10, 96, 1326, 43, 29, 31, 17, 16, 7, 2880, 920, 574, 28, 8, 1508, 5, 96, 11836, 62, 33, 2718, 24, 80, 42, 192, 13, 135, 33, 9805, 12, 1205, 12, 10256, 14159, 1066, 145, 865, 535, 14734, 12, 4712, 2781, 584, 30, 9938, 5061, 10256, 6, 28571, 3, 60, 18, 155, 15, 4094, 112, 3, 8389, 6, 2145, 2627, 1508, 224, 38, 14586, 7677, 423, 18, 1549, 1414, 265, 6060, 11, 411, 7, 2026, 63, 7, 24397, 49, 12446, 2262, 3791, 447, 16, 10256, 225, 240, 20799, 1433, 5, 96, 196, 17, 31, 7, 6865, 3, 9, 1643, 866, 13, 540, 784, 28843, 4275, 37, 7021, 33, 12932, 15436, 13, 24, 1696, 11, 8, 6266, 33, 3, 3131, 3996, 13606, 51, 16, 5, 96, 5231, 34, 31, 7, 3, 9, 792, 815, 13, 131, 147, 23395, 51, 11, 3, 99, 25, 320, 44, 8, 10549, 13, 21331, 24, 8, 233, 3413, 233, 43, 118, 3, 22765, 12, 281, 10055, 21, 784, 355, 908, 1516, 6201, 13, 540, 5, 96, 5231, 3, 99, 62, 130, 12, 830, 8, 1508, 223, 6, 62, 31, 26, 1077, 129, 874, 42, 1296, 1508, 5, 96, 7175, 27, 31, 162, 373, 1800, 3, 18, 11, 48, 19, 28, 82, 22209, 3, 547, 30, 230, 117, 48, 19, 59, 1719, 42, 549, 8503, 3, 18, 27, 31, 26, 1066, 1492, 24, 540, 30, 2627, 1508, 16, 10256, 5, 96, 7238, 33, 1508, 1107, 91, 13, 1696, 6, 2361, 16, 8, 416, 215, 42, 78, 233, 25, 31, 60, 479, 44, 39, 1414, 265, 6060, 31, 13, 8, 296, 117, 12446, 2262, 3791, 447, 21, 677, 3, 18, 62, 174, 12, 453, 175, 3413, 16, 10256, 5, 96, 1326, 700, 241, 135, 132, 5, 328, 33, 8, 2102, 113, 33, 352, 12, 18514, 8, 1021, 1082, 6, 21, 677, 5, 96, 10273, 33, 8, 1843, 13, 17736, 24, 69, 1021, 1082, 241, 12, 29953, 5, 96, 5231, 27, 133, 456, 326, 784, 969, 2145, 908, 28, 8, 1643, 815, 13, 540, 6, 62, 43, 12, 7365, 1508, 16, 10256, 5, 96, 17527, 6, 3, 99, 24, 54, 36, 612, 11, 132, 31, 7, 128, 8179, 3, 26413, 347, 44, 8, 414, 6, 4273, 6, 752, 31, 7, 320, 12, 830, 1508, 223, 5, 96, 11836, 34, 31, 7, 3, 9, 23958, 296, 6, 19, 29, 31, 17, 34, 58, 96, 196, 17, 31, 7, 1399, 12, 240, 8, 3, 13863, 11, 281, 6, 68, 248, 3, 99, 25, 54, 129, 135, 223, 38, 168, 6, 937, 132, 31, 7, 631, 540, 535, 2390, 11, 7262, 10371, 7, 2050, 2715, 7, 65, 16, 15777, 3, 88, 56, 217, 91, 112, 16046, 10730, 1696, 5, 216, 11, 16202, 7, 92, 2283, 19664, 8, 800, 13, 3140, 1919, 5, 2715, 7, 92, 10246, 271, 4781, 57, 2622, 16, 2379, 29494, 301, 31, 427, 23067, 15, 3, 20923, 12, 16046, 9493, 9906, 17, 325, 2360, 822, 53, 70, 9570, 5, 2969, 2715, 7, 11, 24397, 49, 31914, 17, 15, 3311, 16046, 2177, 13, 8, 2038, 11590, 774, 298, 14667, 440, 18, 17114, 16202, 7, 2301, 132, 16, 1882, 2038, 227, 271, 19664, 21, 3, 15471, 2081, 57, 1798, 1886, 2474, 5993, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[368, 22982, 24895, 3545, 13404, 3121, 15, 189, 28571, 7228, 3, 9, 4494, 3996, 19660, 51, 549, 8503, 18, 18145, 7, 3069, 225, 36, 261, 12, 7365, 234, 18, 390, 3683, 224, 38, 1414, 265, 6060, 6, 59, 830, 223, 1215, 699, 26, 4811, 5, 1]]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(raw_datasets['train'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 145\n",
      "text:\n",
      " summarize: Recent reports have linked some France-based players with returns to Wales. \"I've always felt - and this is with my rugby hat on now; this is not region or WRU - I'd rather spend that money on keeping players in Wales,\" said Davies. The WRU provides £2m to the fund and £1.3m comes from the regions. Former Wales and British and Irish Lions fly-half Davies became WRU chairman on Tuesday 21 October, succeeding deposed David Pickering following governing body elections. He is now serving a notice period to leave his role as Newport Gwent Dragons chief executive after being voted on to the WRU board in September. Davies was among the leading figures among Dragons, Ospreys, Scarlets and Cardiff Blues officials who were embroiled in a protracted dispute with the WRU that ended in a £60m deal in August this year. In the wake of that deal being done, Davies said the £3.3m should be spent on ensuring current Wales-based stars remain there. In recent weeks, Racing Metro flanker Dan Lydiate was linked with returning to Wales. Likewise the Paris club's scrum-half Mike Phillips and centre Jamie Roberts were also touted for possible returns. Wales coach Warren Gatland has said: \"We haven't instigated contact with the players. \"But we are aware that one or two of them are keen to return to Wales sooner rather than later.\" Speaking to Scrum V on BBC Radio Wales, Davies re-iterated his stance, saying keeping players such as Scarlets full-back Liam Williams and Ospreys flanker Justin Tipuric in Wales should take precedence. \"It's obviously a limited amount of money [available]. The union are contributing 60% of that contract and the regions are putting £1.3m in. \"So it's a total pot of just over £3m and if you look at the sorts of salaries that the... guys... have been tempted to go overseas for [are] significant amounts of money. \"So if we were to bring the players back, we'd probably get five or six players. \"And I've always felt - and this is with my rugby hat on now; this is not region or WRU - I'd rather spend that money on keeping players in Wales. \"There are players coming out of contract, perhaps in the next year or so... you're looking at your Liam Williams' of the world; Justin Tipuric for example - we need to keep these guys in Wales. \"We actually want them there. They are the ones who are going to impress the young kids, for example. \"They are the sort of heroes that our young kids want to emulate. \"So I would start off [by saying] with the limited pot of money, we have to retain players in Wales. \"Now, if that can be done and there's some spare monies available at the end, yes, let's look to bring players back. \"But it's a cruel world, isn't it? \"It's fine to take the buck and go, but great if you can get them back as well, provided there's enough money.\" British and Irish Lions centre Roberts has insisted he will see out his Racing Metro contract. He and Phillips also earlier dismissed the idea of leaving Paris. Roberts also admitted being hurt by comments in French Newspaper L'Equipe attributed to Racing Coach Laurent Labit questioning their effectiveness. Centre Roberts and flanker Lydiate joined Racing ahead of the 2013-14 season while scrum-half Phillips moved there in December 2013 after being dismissed for disciplinary reasons by former club Bayonne.</s>\n",
      "labels:\n",
      " New Welsh Rugby Union chairman Gareth Davies believes a joint £3.3m WRU-regions fund should be used to retain home-based talent such as Liam Williams, not bring back exiled stars.</s>\n"
     ]
    }
   ],
   "source": [
    "print(\"text:\\n\",tokenizer.decode(preprocess_function(raw_datasets['train'][:1])['input_ids'][0]))\n",
    "print(\"labels:\\n\",tokenizer.decode(preprocess_function(raw_datasets['train'][:1])['labels'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "`dataset` object使用 `map` 方法处理，训练集、验证集、测试集一次性处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /raid/wuxiaojun/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499/cache-d2672c5c9532e4c6.arrow\n",
      "Loading cached processed dataset at /raid/wuxiaojun/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499/cache-90eecd02bce010ce.arrow\n",
      "Loading cached processed dataset at /raid/wuxiaojun/.cache/huggingface/datasets/xsum/default/1.2.0/4957825a982999fbf80bca0b342793b01b2611e021ef589fb7c6250b3577b499/cache-adae3b78978db3b1.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voWiw8C7IrJV"
   },
   "source": [
    "`map`使用 `load_from_cache_file=False` 可以不使用缓存.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "# Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "我们的模型是一个seq2seq模型，所以导入 `AutoModelForSeq2SeqLM` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "使用`Seq2SeqTrainer`, 我们需要定义好[`Seq2SeqTrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"test-summarization\", # 模型保存的位置\n",
    "#     overwrite_output_dir = True,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01, # 权重衰减\n",
    "    save_total_limit=3, # 最多保存三次模型\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True, # 生成摘要\n",
    "    fp16=True, # 激活混合精度训练（以更快一点）\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据整理器，pad the inputs to the maximum length in the batch, but also the labels\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "UmvbnJ9JIrJd"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "Then we just need to pass all of this along with our datasets to the `Seq2SeqTrainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /raid/wuxiaojun/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdzABDVcIrJg"
   },
   "source": [
    "We can now finetune our model by just calling the `train` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "uNx5pyRlIrJh",
    "outputId": "077e661e-d36c-469b-89b8-7ff7f73541ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7975' max='7975' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7975/7975 1:15:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.713700</td>\n",
       "      <td>2.472508</td>\n",
       "      <td>28.399400</td>\n",
       "      <td>7.836000</td>\n",
       "      <td>22.362600</td>\n",
       "      <td>22.366900</td>\n",
       "      <td>18.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.685400</td>\n",
       "      <td>2.450603</td>\n",
       "      <td>28.762200</td>\n",
       "      <td>8.023500</td>\n",
       "      <td>22.661500</td>\n",
       "      <td>22.661000</td>\n",
       "      <td>18.821400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.667600</td>\n",
       "      <td>2.438797</td>\n",
       "      <td>28.866600</td>\n",
       "      <td>8.081800</td>\n",
       "      <td>22.734300</td>\n",
       "      <td>22.734200</td>\n",
       "      <td>18.827000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.658400</td>\n",
       "      <td>2.435523</td>\n",
       "      <td>28.897700</td>\n",
       "      <td>8.123300</td>\n",
       "      <td>22.784000</td>\n",
       "      <td>22.786900</td>\n",
       "      <td>18.823200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.658400</td>\n",
       "      <td>2.435521</td>\n",
       "      <td>28.894500</td>\n",
       "      <td>8.125000</td>\n",
       "      <td>22.786000</td>\n",
       "      <td>22.789000</td>\n",
       "      <td>18.823300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/raid/wuxiaojun/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7975, training_loss=2.68158392927116, metrics={'train_runtime': 4529.7799, 'train_samples_per_second': 225.226, 'train_steps_per_second': 1.761, 'total_flos': 4.0956649065630106e+17, 'train_loss': 2.68158392927116, 'epoch': 5.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 146\n"
     ]
    }
   ],
   "source": [
    "text_example = preprocess_function(raw_datasets['train'][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 150\n"
     ]
    }
   ],
   "source": [
    "# 参考这个文章https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generated_ids = model.generate(\n",
    "                input_ids = torch.Tensor(text_example['input_ids']).to(device, dtype = torch.long),\n",
    "                attention_mask = torch.Tensor(text_example['attention_mask']).to(device, dtype = torch.long), \n",
    "                max_length=150, \n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5, \n",
    "                length_penalty=1.0, \n",
    "                early_stopping=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 151\n"
     ]
    }
   ],
   "source": [
    "preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Newport Gwent Dragons fly-half David Davies has said he would rather spend £3.3m on keeping players in Wales.']\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考链接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- huggingface官方例子：https://github.com/huggingface/notebooks/blob/master/examples/summarization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Summarization",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
