{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T09:24:15.427545Z",
     "start_time": "2021-04-23T09:24:15.015898Z"
    },
    "execution": {
     "iopub.execute_input": "2021-04-24T00:51:33.640418Z",
     "iopub.status.busy": "2021-04-24T00:51:33.639880Z",
     "iopub.status.idle": "2021-04-24T00:51:38.503727Z",
     "shell.execute_reply": "2021-04-24T00:51:38.502437Z",
     "shell.execute_reply.started": "2021-04-24T00:51:33.640371Z"
    }
   },
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T13:34:52.838479Z",
     "start_time": "2021-04-23T13:34:52.455079Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T13:34:54.138977Z",
     "start_time": "2021-04-23T13:34:53.477451Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tag_type = ['O', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC']\n",
    "# B-ORG I-ORG 机构的开始位置和中间位置\n",
    "# B-PER I-PER 人物名字的开始位置和中间位置\n",
    "# B-LOC I-LOC 位置的开始位置和中间位置\n",
    "\n",
    "train_lines = codecs.open('datasets/named_entity_recognition/msra/train/sentences.txt').readlines()\n",
    "train_lines = [x.replace(' ', '').strip() for x in train_lines]\n",
    "\n",
    "train_tags = codecs.open('datasets/named_entity_recognition/msra/train/tags.txt').readlines()\n",
    "train_tags = [x.strip().split(' ') for x in train_tags]\n",
    "train_tags = [[tag_type.index(x) for x in tag] for tag in train_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_lines, train_tags = train_lines[:20000], train_tags[:20000] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T14:53:47.364732Z",
     "start_time": "2021-04-23T14:53:47.359559Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_lines = codecs.open('datasets/named_entity_recognition/msra/val/sentences.txt').readlines()\n",
    "val_lines = [x.replace(' ', '').strip() for x in val_lines]\n",
    "\n",
    "val_tags = codecs.open('datasets/named_entity_recognition/msra/val/tags.txt').readlines()\n",
    "val_tags = [x.strip().split(' ') for x in val_tags]\n",
    "val_tags = [[tag_type.index(x) for x in tag] for tag in val_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('如何解决足球界长期存在的诸多矛盾，重振昔日津门足球的雄风，成为天津足坛上下内外到处议论的话题。',\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines[0], train_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T13:35:14.896288Z",
     "start_time": "2021-04-23T13:35:02.759708Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "train_encoding = tokenizer(list(train_lines), truncation=True, padding=True, max_length=64)\n",
    "val_encoding = tokenizer(list(val_lines), truncation=True, padding=True, max_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T13:35:17.120298Z",
     "start_time": "2021-04-23T13:35:16.751314Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx])[:64] for key, val in self.encodings.items()}\n",
    "        # 字级别的标注\n",
    "        item['labels'] = torch.tensor([0] + self.labels[idx] + [0] * (63-len(self.labels[idx])))[:64]\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = TextDataset(train_encoding, train_tags[:])\n",
    "test_dataset = TextDataset(val_encoding, val_tags[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 101, 1963,  862, 6237, 1104, 6639, 4413, 4518, 7270, 3309, 2100, 1762,\n",
       "         4638, 6436, 1914, 4757, 4688, 8024, 7028, 2920, 3212, 3189, 3823, 7305,\n",
       "         6639, 4413, 4638, 7413, 7599, 8024, 2768,  711, 1921, 3823, 6639, 1781,\n",
       "          677,  678, 1079, 1912, 1168, 1905, 6379, 6389, 4638, 6413, 7579,  511,\n",
       "          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 如 何 解 决 足 球 界 长 期 存 在 的 诸 多 矛 盾 ， 重 振 昔 日 津 门 足 球 的 雄 风 ， 成 为 天 津 足 坛 上 下 内 外 到 处 议 论 的 话 题 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in range(len(train_dataset)):\n",
    "    item = train_dataset[idx]\n",
    "    for key in item:\n",
    "        if item[key].shape[0] != 64:\n",
    "            print(key, item[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in range(len(test_dataset)):\n",
    "    item = test_dataset[idx]\n",
    "    for key in item:\n",
    "        if item[key].shape[0] != 64:\n",
    "            print(key, item[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-chinese', num_labels=7)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# BertForTokenClassification只是一个字级别的分类器\n",
    "# 想要使用bert+crf可以参考这个实现\n",
    "# https://github.com/Louis-udm/NER-BERT-CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T13:35:18.744186Z",
     "start_time": "2021-04-23T13:35:18.183277Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "total_steps = len(train_loader) * 1\n",
    "scheduler = get_linear_schedule_with_warmup(optim, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T13:37:36.436445Z",
     "start_time": "2021-04-23T13:35:34.993720Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Epoch: 0 ----------------\n",
      "0.17626953125 1.965804100036621\n",
      "0.966796875 0.13332447409629822\n",
      "0.9814453125 0.09005818516016006\n",
      "0.970703125 0.09516040235757828\n",
      "0.97021484375 0.14700622856616974\n",
      "epoth: 0, iter_num: 100, loss: 0.1206, 16.00%\n",
      "0.97412109375 0.12301269173622131\n",
      "0.966796875 0.13751883804798126\n",
      "0.96923828125 0.12724775075912476\n",
      "0.97265625 0.11036788672208786\n",
      "0.98388671875 0.07470635324716568\n",
      "epoth: 0, iter_num: 200, loss: 0.0808, 32.00%\n",
      "0.99658203125 0.02434217929840088\n",
      "0.9775390625 0.07938043028116226\n",
      "0.97998046875 0.07676959782838821\n",
      "0.98779296875 0.06868980824947357\n",
      "0.98583984375 0.05229904130101204\n",
      "epoth: 0, iter_num: 300, loss: 0.0468, 48.00%\n",
      "0.9873046875 0.05262850970029831\n",
      "0.98876953125 0.06584114581346512\n",
      "0.98583984375 0.03852091357111931\n",
      "0.98779296875 0.051144976168870926\n",
      "0.97998046875 0.0400601401925087\n",
      "epoth: 0, iter_num: 400, loss: 0.0643, 64.00%\n",
      "0.98583984375 0.054247722029685974\n",
      "0.9892578125 0.032272450625896454\n",
      "0.9853515625 0.06257447600364685\n",
      "0.9833984375 0.05568825080990791\n",
      "0.98583984375 0.11944360285997391\n",
      "epoth: 0, iter_num: 500, loss: 0.0301, 80.00%\n",
      "0.990234375 0.051323141902685165\n",
      "0.98193359375 0.0614640899002552\n",
      "0.9931640625 0.020426657050848007\n",
      "0.98583984375 0.05529442057013512\n",
      "0.99365234375 0.020422540605068207\n",
      "epoth: 0, iter_num: 600, loss: 0.0790, 96.00%\n",
      "0.97265625 0.20828479528427124\n",
      "0.9814453125 0.059993598610162735\n",
      "Epoch: 0, Average training loss: 0.0814\n",
      "Accuracy: 0.9902\n",
      "Average testing loss: 0.0392\n",
      "-------------------------------\n",
      "------------Epoch: 1 ----------------\n",
      "0.9873046875 0.07896505296230316\n",
      "0.99560546875 0.024551676586270332\n",
      "0.99560546875 0.02059563435614109\n",
      "0.99951171875 0.008247436955571175\n",
      "0.9921875 0.02587205357849598\n",
      "epoth: 1, iter_num: 100, loss: 0.0257, 16.00%\n",
      "0.99267578125 0.019116872921586037\n",
      "0.9873046875 0.12489720433950424\n",
      "0.99072265625 0.032339248806238174\n",
      "0.9951171875 0.021087663248181343\n",
      "0.9921875 0.02596503123641014\n",
      "epoth: 1, iter_num: 200, loss: 0.0147, 32.00%\n",
      "0.98193359375 0.09586840867996216\n",
      "0.99365234375 0.022845802828669548\n",
      "0.98828125 0.03298266604542732\n",
      "0.99658203125 0.005134826526045799\n",
      "0.9931640625 0.027017297223210335\n",
      "epoth: 1, iter_num: 300, loss: 0.0159, 48.00%\n",
      "0.99267578125 0.023266911506652832\n",
      "0.99267578125 0.018210772424936295\n",
      "0.9951171875 0.012915805913507938\n",
      "0.98876953125 0.03161023557186127\n",
      "0.9833984375 0.06870781630277634\n",
      "epoth: 1, iter_num: 400, loss: 0.0382, 64.00%\n",
      "0.99365234375 0.027863597497344017\n",
      "0.97802734375 0.08203934878110886\n",
      "0.99560546875 0.024993283674120903\n",
      "0.9921875 0.023670269176363945\n",
      "0.9921875 0.04846701771020889\n",
      "epoth: 1, iter_num: 500, loss: 0.0392, 80.00%\n",
      "0.9912109375 0.017621750012040138\n",
      "0.99365234375 0.013802383095026016\n",
      "0.99365234375 0.0253745187073946\n",
      "0.98974609375 0.027173448354005814\n",
      "0.9921875 0.03262724354863167\n",
      "epoth: 1, iter_num: 600, loss: 0.0093, 96.00%\n",
      "0.99560546875 0.010945513844490051\n",
      "0.9970703125 0.01727929897606373\n",
      "Epoch: 1, Average training loss: 0.0317\n",
      "Accuracy: 0.9902\n",
      "Average testing loss: 0.0392\n",
      "-------------------------------\n",
      "------------Epoch: 2 ----------------\n",
      "0.990234375 0.024441363289952278\n",
      "0.9921875 0.03158508986234665\n",
      "0.99365234375 0.02776366099715233\n",
      "0.98486328125 0.060661882162094116\n",
      "0.9921875 0.009924284182488918\n",
      "epoth: 2, iter_num: 100, loss: 0.0352, 16.00%\n",
      "0.99951171875 0.008196990005671978\n",
      "0.9912109375 0.027444424107670784\n",
      "0.990234375 0.03357464447617531\n",
      "0.99267578125 0.02302759326994419\n",
      "0.986328125 0.058670368045568466\n",
      "epoth: 2, iter_num: 200, loss: 0.0283, 32.00%\n",
      "0.99560546875 0.016662219539284706\n",
      "0.99365234375 0.026256393641233444\n",
      "0.9951171875 0.022496772930026054\n",
      "0.99462890625 0.01676914095878601\n",
      "0.982421875 0.037706874310970306\n",
      "epoth: 2, iter_num: 300, loss: 0.0351, 48.00%\n",
      "0.98974609375 0.0341406874358654\n",
      "0.9931640625 0.014812282286584377\n",
      "0.99609375 0.01548784039914608\n",
      "0.99755859375 0.004309654701501131\n",
      "0.9931640625 0.019518228247761726\n",
      "epoth: 2, iter_num: 400, loss: 0.0403, 64.00%\n",
      "0.99658203125 0.013795559294521809\n",
      "0.99755859375 0.016307014971971512\n",
      "0.9931640625 0.022853145375847816\n",
      "0.990234375 0.03537184000015259\n",
      "0.982421875 0.049064815044403076\n",
      "epoth: 2, iter_num: 500, loss: 0.0149, 80.00%\n",
      "0.9921875 0.03463701158761978\n",
      "0.99462890625 0.01944301277399063\n",
      "0.98876953125 0.021545208990573883\n",
      "0.98828125 0.07002919912338257\n",
      "0.99267578125 0.02153755910694599\n",
      "epoth: 2, iter_num: 600, loss: 0.0405, 96.00%\n",
      "0.98828125 0.045930538326501846\n",
      "0.99853515625 0.014012620784342289\n",
      "Epoch: 2, Average training loss: 0.0314\n",
      "Accuracy: 0.9901\n",
      "Average testing loss: 0.0392\n",
      "-------------------------------\n",
      "------------Epoch: 3 ----------------\n",
      "0.994140625 0.0246267206966877\n",
      "0.99609375 0.017024816945195198\n",
      "0.98974609375 0.03854893520474434\n",
      "0.990234375 0.03946729749441147\n",
      "0.994140625 0.018616998568177223\n",
      "epoth: 3, iter_num: 100, loss: 0.0204, 16.00%\n",
      "0.984375 0.020612889900803566\n",
      "0.990234375 0.028798451647162437\n",
      "0.9951171875 0.01597573235630989\n",
      "0.98828125 0.029347579926252365\n",
      "0.98974609375 0.0252370648086071\n",
      "epoth: 3, iter_num: 200, loss: 0.0126, 32.00%\n",
      "0.97998046875 0.06245586276054382\n",
      "0.98291015625 0.04890350624918938\n",
      "0.99169921875 0.03774634003639221\n",
      "0.99609375 0.013897735625505447\n",
      "0.994140625 0.040432315319776535\n",
      "epoth: 3, iter_num: 300, loss: 0.0435, 48.00%\n",
      "0.98876953125 0.03544748201966286\n",
      "0.98291015625 0.06015520915389061\n",
      "0.9853515625 0.023523323237895966\n",
      "0.99462890625 0.020694859325885773\n",
      "0.99560546875 0.011159862391650677\n",
      "epoth: 3, iter_num: 400, loss: 0.0506, 64.00%\n",
      "0.99072265625 0.029803412035107613\n",
      "0.99072265625 0.03717641159892082\n",
      "0.98583984375 0.07219034433364868\n",
      "0.98583984375 0.05318031460046768\n",
      "0.99462890625 0.016858991235494614\n",
      "epoth: 3, iter_num: 500, loss: 0.0053, 80.00%\n",
      "0.99365234375 0.02699870988726616\n",
      "0.9833984375 0.048285987228155136\n",
      "0.994140625 0.014430229552090168\n",
      "0.990234375 0.021233752369880676\n",
      "0.99365234375 0.022245122119784355\n",
      "epoth: 3, iter_num: 600, loss: 0.0623, 96.00%\n",
      "0.98974609375 0.03245074674487114\n",
      "0.98876953125 0.05311824381351471\n",
      "Epoch: 3, Average training loss: 0.0318\n",
      "Accuracy: 0.9901\n",
      "Average testing loss: 0.0390\n",
      "-------------------------------\n",
      "------------Epoch: 4 ----------------\n",
      "0.98388671875 0.04762868583202362\n",
      "0.99462890625 0.0262977946549654\n",
      "0.990234375 0.0216385405510664\n",
      "0.98388671875 0.07145150005817413\n",
      "0.98974609375 0.03908342123031616\n",
      "epoth: 4, iter_num: 100, loss: 0.0399, 16.00%\n",
      "0.99609375 0.014670521020889282\n",
      "0.98974609375 0.04279158636927605\n",
      "0.99169921875 0.013748863711953163\n",
      "0.98974609375 0.057699788361787796\n",
      "0.99072265625 0.028676342219114304\n",
      "epoth: 4, iter_num: 200, loss: 0.0166, 32.00%\n",
      "0.9833984375 0.08263783901929855\n",
      "0.98828125 0.027033722028136253\n",
      "0.99072265625 0.028177563101053238\n",
      "0.99560546875 0.017070293426513672\n",
      "0.98046875 0.045836057513952255\n",
      "epoth: 4, iter_num: 300, loss: 0.0372, 48.00%\n",
      "0.99072265625 0.02906404249370098\n",
      "0.98876953125 0.008922277018427849\n",
      "0.990234375 0.014935425482690334\n",
      "0.990234375 0.04827646166086197\n",
      "0.990234375 0.021689055487513542\n",
      "epoth: 4, iter_num: 400, loss: 0.0338, 64.00%\n",
      "0.99560546875 0.02491515874862671\n",
      "0.99951171875 0.00651667732745409\n",
      "0.99609375 0.011120226234197617\n",
      "0.98974609375 0.026097100228071213\n",
      "0.99609375 0.026907287538051605\n",
      "epoth: 4, iter_num: 500, loss: 0.0662, 80.00%\n",
      "0.99365234375 0.025796590372920036\n",
      "0.9892578125 0.040862683206796646\n",
      "0.98876953125 0.020262859761714935\n",
      "0.9912109375 0.022765403613448143\n",
      "0.9951171875 0.025000136345624924\n",
      "epoth: 4, iter_num: 600, loss: 0.1240, 96.00%\n",
      "0.99365234375 0.016593191772699356\n",
      "0.98828125 0.041193637996912\n",
      "Epoch: 4, Average training loss: 0.0317\n",
      "Accuracy: 0.9902\n",
      "Average testing loss: 0.0393\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    iter_num = 0\n",
    "    total_iter = len(train_loader)\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            # loss = outputs[0]\n",
    "\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        if idx % 20 == 0:\n",
    "            with torch.no_grad():\n",
    "                # 64 * 7\n",
    "                print((outputs[1].argmax(2).data == labels.data).float().mean().item(), loss.item())\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        iter_num += 1\n",
    "        if(iter_num % 100==0):\n",
    "            print(\"epoth: %d, iter_num: %d, loss: %.4f, %.2f%%\" % (epoch, iter_num, loss.item(), iter_num/total_iter*100))\n",
    "        \n",
    "    print(\"Epoch: %d, Average training loss: %.4f\"%(epoch, total_train_loss/len(train_loader)))\n",
    "    \n",
    "def validation():\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    for batch in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs[1]\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += (outputs[1].argmax(2).data == labels.data).float().mean().item()\n",
    "        \n",
    "    avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "    print(\"Accuracy: %.4f\" % (avg_val_accuracy))\n",
    "    print(\"Average testing loss: %.4f\"%(total_eval_loss/len(test_dataloader)))\n",
    "    print(\"-------------------------------\")\n",
    "    \n",
    "\n",
    "for epoch in range(5):\n",
    "    print(\"------------Epoch: %d ----------------\" % epoch)\n",
    "    train()\n",
    "    validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-24T02:45:11.533512Z",
     "iopub.status.busy": "2021-04-24T02:45:11.532931Z",
     "iopub.status.idle": "2021-04-24T02:45:12.163488Z",
     "shell.execute_reply": "2021-04-24T02:45:12.162999Z",
     "shell.execute_reply.started": "2021-04-24T02:45:11.533464Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'bert-ner.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T14:51:18.819401Z",
     "start_time": "2021-04-23T14:51:18.807242Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tag_type = ['O', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC']\n",
    "\n",
    "def predcit(s):\n",
    "    item = tokenizer([s], truncation=True, padding='longest', max_length=64)\n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.tensor(item['input_ids']).to(device).reshape(1, -1)\n",
    "        attention_mask = torch.tensor(item['attention_mask']).to(device).reshape(1, -1)\n",
    "        labels = torch.tensor([0] * attention_mask.shape[1]).to(device).reshape(1, -1)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, labels)\n",
    "        outputs = outputs[0].data.cpu().numpy()\n",
    "        \n",
    "    outputs = outputs[0].argmax(1)[1:-1]\n",
    "    ner_result = ''\n",
    "    ner_flag = ''\n",
    "    \n",
    "    for o, c in zip(outputs,s):\n",
    "        # 0 就是 O，没有含义\n",
    "        if o == 0 and ner_result == '':\n",
    "            continue\n",
    "        \n",
    "        # \n",
    "        elif o == 0 and ner_result != '':\n",
    "            if ner_flag == 'O':\n",
    "                print('机构：', ner_result)\n",
    "            if ner_flag == 'P':\n",
    "                print('人名：', ner_result)\n",
    "            if ner_flag == 'L':\n",
    "                print('位置：', ner_result)\n",
    "                \n",
    "            ner_result = ''\n",
    "        \n",
    "        elif o != 0:\n",
    "            ner_flag = tag_type[o][2]\n",
    "            ner_result += c\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T14:54:57.736608Z",
     "start_time": "2021-04-23T14:54:57.706272Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "位置： 华盛顿\n",
      "位置： 美国总统府白宫\n",
      "位置： 菲律宾总统府马拉卡南宫\n"
     ]
    }
   ],
   "source": [
    "s = '整个华盛顿已笼罩在一片夜色之中，一个电话从美国总统府白宫打到了菲律宾总统府马拉卡南宫。'\n",
    "data = predcit(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T14:54:58.297655Z",
     "start_time": "2021-04-23T14:54:58.291706Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "位置： 中国\n",
      "位置： 美国\n"
     ]
    }
   ],
   "source": [
    "s = '人工智能是未来的希望，也是中国和美国的冲突点。'\n",
    "data = predcit(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T14:54:42.166525Z",
     "start_time": "2021-04-23T14:54:42.134935Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "位置： 海淀\n",
      "人名： 刘涛\n",
      "人名： 王华\n"
     ]
    }
   ],
   "source": [
    "s = '明天我们一起在海淀吃个饭吧，把叫刘涛和王华也叫上。'\n",
    "data = predcit(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "机构： 同煤集团同生安平煤业公司\n"
     ]
    }
   ],
   "source": [
    "s = '同煤集团同生安平煤业公司发生井下安全事故 19名矿工遇难'\n",
    "data = predcit(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "机构： 山东省政府办公厅\n",
      "机构： 平邑县玉荣商贸有限公司\n"
     ]
    }
   ],
   "source": [
    "s = '山东省政府办公厅就平邑县玉荣商贸有限公司石膏矿坍塌事故发出通报'\n",
    "data = predcit(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "位置： 黑龙江\n",
      "机构： 龙煤集团\n"
     ]
    }
   ],
   "source": [
    "s = '[新闻直播间]黑龙江:龙煤集团一煤矿发生火灾事故'\n",
    "data = predcit(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考链接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://mp.weixin.qq.com/s?__biz=MzIwNDA5NDYzNA==&mid=2247490973&idx=1&sn=d5283ca0889d813d8a32d4829e833fa6&chksm=96c43058a1b3b94e93cbe185cdfbcda3638fb3ef9d3dd25953f9b2e1ecd703e7199e608201e6&scene=178&cur_album_id=1364202321906941952#rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}