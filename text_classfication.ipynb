{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-24T06:31:36.201718Z",
     "iopub.status.busy": "2021-04-24T06:31:36.201147Z",
     "iopub.status.idle": "2021-04-24T06:31:36.998609Z",
     "shell.execute_reply": "2021-04-24T06:31:36.998079Z",
     "shell.execute_reply.started": "2021-04-24T06:31:36.201668Z"
    },
    "tags": []
   },
   "source": [
    "# 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T09:35:08.358976Z",
     "start_time": "2021-03-11T09:35:08.347065Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# 指定使用的gpu\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'体育学': 0, '图书馆、情报与档案管理': 1, '法学': 2}\n",
      "{0: '体育学', 1: '图书馆、情报与档案管理', 2: '法学'}\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14386 entries, 0 to 14385\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   label     14386 non-null  object\n",
      " 1   text      14386 non-null  object\n",
      " 2   label_id  14386 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 337.3+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>法学</td>\n",
       "      <td>域外简易程序在适用范围上主要适用于罪行较轻的犯罪,在程序类型上普遍呈现出多样化的特点,在具体...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>法学</td>\n",
       "      <td>政府非税收入是中国财政收入的重要组成部分,然而其理论研究严重落后于现实需要。文章提出应创建\"...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>法学</td>\n",
       "      <td>集团仲裁是美国为消费者、雇佣、金融等格式合同的弱方当事人集合维权创设的一种纠纷解决方式。美国...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>法学</td>\n",
       "      <td>长期以来,以政府为主导的犯罪打击模式在我国犯罪治理中居于主导地位,但这种模式忽视了被害人等其...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>法学</td>\n",
       "      <td>我国的有限责任公司盈余分配纠纷是有限责任公司股权纠纷的典型争议之一。经济欠发达地区的法院明显...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  label_id\n",
       "0    法学  域外简易程序在适用范围上主要适用于罪行较轻的犯罪,在程序类型上普遍呈现出多样化的特点,在具体...         2\n",
       "1    法学  政府非税收入是中国财政收入的重要组成部分,然而其理论研究严重落后于现实需要。文章提出应创建\"...         2\n",
       "2    法学  集团仲裁是美国为消费者、雇佣、金融等格式合同的弱方当事人集合维权创设的一种纠纷解决方式。美国...         2\n",
       "3    法学  长期以来,以政府为主导的犯罪打击模式在我国犯罪治理中居于主导地位,但这种模式忽视了被害人等其...         2\n",
       "4    法学  我国的有限责任公司盈余分配纠纷是有限责任公司股权纠纷的典型争议之一。经济欠发达地区的法院明显...         2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取数据\n",
    "all_data = pd.read_csv(\"datasets/text_classification/图书馆、情报与档案管理_体育学_法学.csv\")\n",
    "\n",
    "# 只使用摘要和学科\n",
    "all_data = all_data[['学科','摘要']]\n",
    "all_data.columns = ['label','text']\n",
    "\n",
    "# 对学科进行label encoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(all_data['label'])\n",
    "label2id = dict(zip(le.classes_, range(0, len(le.classes_))))\n",
    "id2label = dict(map(reversed, label2id.items()))\n",
    "print(label2id)\n",
    "print(id2label)\n",
    "all_data['label_id'] = all_data['label'].map(label2id)\n",
    "\n",
    "print(all_data.info())\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T09:35:09.263768Z",
     "start_time": "2021-03-11T09:35:09.227357Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 划分为训练集和验证集\n",
    "x_train, x_test, train_label, test_label =  train_test_split(all_data['text'], \n",
    "                                                             all_data['label_id'], \n",
    "                                                             test_size=0.2, \n",
    "                                                             random_state = 123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2\n",
       "1        0\n",
       "2        0\n",
       "3        2\n",
       "4        0\n",
       "        ..\n",
       "11503    0\n",
       "11504    1\n",
       "11505    2\n",
       "11506    1\n",
       "11507    2\n",
       "Name: label_id, Length: 11508, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重新索引\n",
    "x_train, x_test, train_label, test_label = x_train.reset_index(drop=True), x_test.reset_index(drop=True), train_label.reset_index(drop=True), test_label.reset_index(drop=True)\n",
    "\n",
    "train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T09:35:19.935035Z",
     "start_time": "2021-03-11T09:35:09.908638Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install transformers\n",
    "# transformers bert相关的模型使用和加载\n",
    "from transformers import BertTokenizer\n",
    "# 分词器，词典\n",
    "\n",
    "# 第一次使用会下载bert-base-chinese到本地缓存，\n",
    "# 或者自己到huggingface官网下载模型，这里填写模型路径\n",
    "# 不想翻墙的话到https://github.com/ymcui/Chinese-BERT-wwm下载哈工大开源的版本\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "train_encoding = tokenizer(list(x_train), truncation=True, padding=True, max_length=128)\n",
    "test_encoding = tokenizer(list(x_test), truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7270, 3309, 809, 3341, 8024, 5401, 1744, 2792, 3354, 2456, 4638, 809, 6887, 2548, 7344, 5576, 510, 3791, 2526, 3780, 5576, 510, 4664, 4719, 2829, 5576, 711, 4294, 2519, 4638, 5468, 6930, 3124, 2424, 4989, 860, 1353, 5576, 6571, 3322, 1169, 2190, 3300, 3126, 7564, 7344, 1469, 6883, 1169, 5576, 6571, 6629, 1168, 749, 1068, 7241, 868, 4500, 511, 2968, 4955, 5401, 1744, 5468, 6930, 3124, 2424, 1741, 5312, 794, 3124, 6887, 2548, 1469, 6121, 3124, 840, 4415, 2456, 6392, 2792, 6822, 6121, 4638, 4989, 3791, 1469, 2809, 3791, 2141, 6664, 8024, 5440, 2175, 1071, 6887, 2548, 3780, 5576, 6817, 6121, 3322, 1169, 8024, 2190, 2769, 1744, 1217, 2571, 3354, 2456, 5143, 5320, 510, 6226, 5745, 510, 3403, 1114, 4638, 6121, 3124, 840, 4415, 1169, 2428, 1265, 860, 5143, 510, 2130, 1587, 6887, 2548, 5276, 3338, 680, 3791, 3780, 2674, 2770, 4685, 5310, 1394, 4638, 1353, 5576, 3322, 1169, 1072, 3300, 671, 2137, 4638, 955, 7063, 2692, 721, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} [CLS] 长 期 以 来 ， 美 国 所 构 建 的 以 道 德 防 腐 、 法 律 治 腐 、 监 督 抑 腐 为 特 征 的 联 邦 政 府 立 体 反 腐 败 机 制 对 有 效 预 防 和 遏 制 腐 败 起 到 了 关 键 作 用 。 探 究 美 国 联 邦 政 府 围 绕 从 政 道 德 和 行 政 伦 理 建 设 所 进 行 的 立 法 和 执 法 实 践 ， 考 察 其 道 德 治 腐 运 行 机 制 ， 对 我 国 加 快 构 建 系 统 、 规 范 、 标 准 的 行 政 伦 理 制 度 化 体 系 、 完 善 道 德 约 束 与 法 治 惩 戒 相 结 合 的 反 腐 机 制 具 有 一 定 的 借 鉴 意 义 。 [SEP]\n"
     ]
    }
   ],
   "source": [
    "# input_ids：字的编码\n",
    "# token_type_ids：标识是第一个句子还是第二个句子\n",
    "# attention_mask：标识是不是填充, 是不是可以被attention到\n",
    "encode_example = tokenizer(x_train[0])\n",
    "decode_example = tokenizer.decode(encode_example.input_ids)\n",
    "\n",
    "print(encode_example, decode_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T09:35:43.578135Z",
     "start_time": "2021-03-11T09:35:43.571452Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 数据集读取\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    # 读取单个样本\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(int(self.labels[idx]))\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = NewsDataset(train_encoding, train_label)\n",
    "test_dataset = NewsDataset(test_encoding, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 101, 7270, 3309,  809, 3341, 8024, 5401, 1744, 2792, 3354, 2456, 4638,\n",
       "          809, 6887, 2548, 7344, 5576,  510, 3791, 2526, 3780, 5576,  510, 4664,\n",
       "         4719, 2829, 5576,  711, 4294, 2519, 4638, 5468, 6930, 3124, 2424, 4989,\n",
       "          860, 1353, 5576, 6571, 3322, 1169, 2190, 3300, 3126, 7564, 7344, 1469,\n",
       "         6883, 1169, 5576, 6571, 6629, 1168,  749, 1068, 7241,  868, 4500,  511,\n",
       "         2968, 4955, 5401, 1744, 5468, 6930, 3124, 2424, 1741, 5312,  794, 3124,\n",
       "         6887, 2548, 1469, 6121, 3124,  840, 4415, 2456, 6392, 2792, 6822, 6121,\n",
       "         4638, 4989, 3791, 1469, 2809, 3791, 2141, 6664, 8024, 5440, 2175, 1071,\n",
       "         6887, 2548, 3780, 5576, 6817, 6121, 3322, 1169, 8024, 2190, 2769, 1744,\n",
       "         1217, 2571, 3354, 2456, 5143, 5320,  510, 6226, 5745,  510, 3403, 1114,\n",
       "         4638, 6121, 3124,  840, 4415, 1169, 2428,  102]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor(2)}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T09:35:44.110121Z",
     "start_time": "2021-03-11T09:35:44.104871Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 精度计算\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载预训练模型进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T09:58:49.027161Z",
     "start_time": "2021-03-11T09:58:45.317009Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=3)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 单个读取到批量读取\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# 优化方法\n",
    "optim = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_loader) * 1\n",
    "scheduler = get_linear_schedule_with_warmup(optim, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T09:44:22.077501Z",
     "start_time": "2021-03-11T09:39:16.473609Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Epoch: 0 ----------------\n",
      "epoth: 0, iter_num: 100, loss: 0.0134, 13.89%\n",
      "epoth: 0, iter_num: 200, loss: 0.0046, 27.78%\n",
      "epoth: 0, iter_num: 300, loss: 0.0037, 41.67%\n",
      "epoth: 0, iter_num: 400, loss: 0.0022, 55.56%\n",
      "epoth: 0, iter_num: 500, loss: 0.0020, 69.44%\n",
      "epoth: 0, iter_num: 600, loss: 0.0045, 83.33%\n",
      "epoth: 0, iter_num: 700, loss: 0.0022, 97.22%\n",
      "Epoch: 0, Average training loss: 0.1065\n",
      "Accuracy: 0.9854\n",
      "Average testing loss: 0.0688\n",
      "-------------------------------\n",
      "------------Epoch: 1 ----------------\n",
      "epoth: 1, iter_num: 100, loss: 0.3073, 13.89%\n",
      "epoth: 1, iter_num: 200, loss: 0.0021, 27.78%\n",
      "epoth: 1, iter_num: 300, loss: 0.0017, 41.67%\n",
      "epoth: 1, iter_num: 400, loss: 0.0043, 55.56%\n",
      "epoth: 1, iter_num: 500, loss: 0.0036, 69.44%\n",
      "epoth: 1, iter_num: 600, loss: 0.0024, 83.33%\n",
      "epoth: 1, iter_num: 700, loss: 0.0033, 97.22%\n",
      "Epoch: 1, Average training loss: 0.0381\n",
      "Accuracy: 0.9854\n",
      "Average testing loss: 0.0688\n",
      "-------------------------------\n",
      "------------Epoch: 2 ----------------\n",
      "epoth: 2, iter_num: 100, loss: 0.0019, 13.89%\n",
      "epoth: 2, iter_num: 200, loss: 0.0022, 27.78%\n",
      "epoth: 2, iter_num: 300, loss: 0.0019, 41.67%\n",
      "epoth: 2, iter_num: 400, loss: 0.0058, 55.56%\n",
      "epoth: 2, iter_num: 500, loss: 0.0059, 69.44%\n",
      "epoth: 2, iter_num: 600, loss: 0.0019, 83.33%\n",
      "epoth: 2, iter_num: 700, loss: 0.0098, 97.22%\n",
      "Epoch: 2, Average training loss: 0.0382\n",
      "Accuracy: 0.9854\n",
      "Average testing loss: 0.0689\n",
      "-------------------------------\n",
      "------------Epoch: 3 ----------------\n",
      "epoth: 3, iter_num: 100, loss: 0.0021, 13.89%\n",
      "epoth: 3, iter_num: 200, loss: 0.0015, 27.78%\n",
      "epoth: 3, iter_num: 300, loss: 0.0021, 41.67%\n",
      "epoth: 3, iter_num: 400, loss: 0.0018, 55.56%\n",
      "epoth: 3, iter_num: 500, loss: 0.0024, 69.44%\n",
      "epoth: 3, iter_num: 600, loss: 0.0076, 83.33%\n",
      "epoth: 3, iter_num: 700, loss: 0.0036, 97.22%\n",
      "Epoch: 3, Average training loss: 0.0387\n",
      "Accuracy: 0.9854\n",
      "Average testing loss: 0.0688\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 训练函数\n",
    "def train():\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    iter_num = 0\n",
    "    total_iter = len(train_loader)\n",
    "    for batch in train_loader:\n",
    "        # 正向传播\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # 反向梯度信息\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # 参数更新\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        iter_num += 1\n",
    "        if(iter_num % 100==0):\n",
    "            print(\"epoth: %d, iter_num: %d, loss: %.4f, %.2f%%\" % (epoch, iter_num, loss.item(), iter_num/total_iter*100))\n",
    "        \n",
    "    print(\"Epoch: %d, Average training loss: %.4f\"%(epoch, total_train_loss/len(train_loader)))\n",
    "    \n",
    "def validation():\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    for batch in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            # 正常传播\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "    avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "    print(\"Accuracy: %.4f\" % (avg_val_accuracy))\n",
    "    print(\"Average testing loss: %.4f\"%(total_eval_loss/len(test_dataloader)))\n",
    "    print(\"-------------------------------\")\n",
    "    \n",
    "\n",
    "# 训练4个epochs\n",
    "for epoch in range(4):\n",
    "    print(\"------------Epoch: %d ----------------\" % epoch)\n",
    "    train()\n",
    "    validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1、只保存模型参数\n",
    "# 保存\n",
    "torch.save(model.state_dict(), '\\parameter.pkl')\n",
    "# 加载\n",
    "model = TheModelClass(...)\n",
    "model.load_state_dict(torch.load('\\parameter.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2、保存完整模型\n",
    "# 保存\n",
    "torch.save(model, '\\text_classification_model.pkl')\n",
    "# 加载\n",
    "# model = torch.load('\\model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-3.1282,  3.8290, -1.9724],\n",
      "        [-2.0875, -1.7345,  5.3452],\n",
      "        [ 5.5092, -2.2844, -1.1295]], device='cuda:0'), hidden_states=None, attentions=None)\n",
      "tensor([1, 2, 0], device='cuda:0')\n",
      "['图书馆、情报与档案管理', '法学', '体育学']\n"
     ]
    }
   ],
   "source": [
    "predict_example = ['大数据时代的小数据因其个体化特征优势在信息资源个性化服务过程中发挥着无可比拟的优势',\n",
    "                   '高校校规是高等学校公共权力行使的重要依据,是现代大学治理的介质文本,在法律地位上可类同于\"规章以下的规范性文件\";',\n",
    "                   '为深入发掘足球运动多元社会功能,争取更多社会共识支持,服务社会发展进步,助力中央足球发展改革战略,运用文献资料、逻辑分析等研究方法,根据综合国力构成要素,从政治、经济、军事、教育、文化、科技6个角度分析足球运动发展倒逼中国社会全面改革的价值意义,认为:1)足球运动可以代替战争这一人类社会竞争最高形式,为社会发展提供变革动力,通过变革促进社会资源整合利用效率提高,在建设、传播国家文化软实力,扩大对外友好交流同时,培养奋发向上的民族意志和凝聚力,服务综合国力整体提升;2)从成功所需因素角度,在相同地缘、社会文化和外交联盟背景下,奥运象征核战力,足球象征常规战力,奥运和足球两种\"和平年代的战争\",对社会发展具有不同促进作用,奥运激励精英先富,足球要求全民共富。美国常规战力要由其美式足球运动和女子足球所获6个世界冠军体现。'\n",
    "                  ]\n",
    "predict_example = tokenizer(predict_example, padding=True, max_length=128)\n",
    "# print(predict_example)\n",
    "with torch.no_grad():\n",
    "        # 正常传播\n",
    "        input_ids = torch.tensor(predict_example['input_ids']).to(device)\n",
    "        attention_mask =  torch.tensor(predict_example['attention_mask']).to(device)\n",
    "#         labels = predict_example['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        print(outputs)\n",
    "        result = outputs['logits'].argmax(axis=1)\n",
    "        print(result)\n",
    "\n",
    "print([id2label[int(i)] for i in list(result)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
